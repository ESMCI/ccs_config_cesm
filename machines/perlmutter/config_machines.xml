  <machine MACH="perlmutter">
    <DESC>NERSC EX AMD EPYC, os is CNL, 64 pes/node, batch system is Slurm</DESC>
    <OS>CNL</OS>
    <COMPILERS>intel,gnu,cray,nvidia,aocc</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>mp9_g</PROJECT>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/global/cfs/cdirs/ccsm1/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/cfs/cdirs/ccsm1/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/global/cfs/cdirs/ccsm1/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/global/cfs/cdirs/ccsm1/tools/cprnc.perlmutter/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_GPUS_PER_NODE>4</MAX_GPUS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>64</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="num_tasks" > -n {{ total_tasks }}</arg>
        <arg name="binding"> -c {{ srun_binding }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/usr/share/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="unload">PrgEnv-intel</command>
        <command name="unload">PrgEnv-nvidia</command>
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-aocc</command>
        <command name="unload">gcc-native</command>
        <command name="unload">intel</command>
        <command name="unload">intel-oneapi</command>
        <command name="unload">nvidia</command>
        <command name="unload">aocc</command>
        <command name="unload">cudatoolkit</command>
        <command name="unload">climate-utils</command>
        <command name="unload">cray-libsci</command>
        <command name="unload">matlab</command>
        <command name="unload">craype-accel-nvidia80</command>
        <command name="unload">craype-accel-host</command>
        <command name="unload">perftools-base</command>
        <command name="unload">perftools</command>
        <command name="unload">darshan</command>
      </modules>

      <modules compiler="intel">
	<command name="load">PrgEnv-intel</command>
      </modules>
      
      <modules compiler="cray">
        <command name="load">PrgEnv-cray</command>
        <command name="switch">cce cce/12.0.3</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">PrgEnv-gnu</command>
	
      </modules>
      <modules>
	<command name="load">craype-x86-milan</command>
        <command name="switch">cray-libsci/23.09.1.1</command>
      </modules>
      <modules>
        <command name="load">cray-mpich/8.1.28</command>
      </modules>
      <modules mpilib="mpi-serial">
        <command name="rm">cray-netcdf-hdf5parallel</command>
        <command name="rm">cray-hdf5-parallel</command>
        <command name="rm">cray-parallel-netcdf</command>
        <command name="load">cray-hdf5</command>
        <command name="load">cray-netcdf</command>
      </modules>
      <modules mpilib="!mpi-serial">
        <command name="load">cray-hdf5-parallel</command>
        <command name="load">cray-netcdf-hdf5parallel</command>
        <command name="load">cray-parallel-netcdf</command>
      </modules>
      <modules>
        <command name="load">cmake/3.24.3</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="ESMFMKFILE">/global/cfs/cdirs/ccsm1/esmf//lib/libO/Unicos.intel.64.mpi.default/esmf.mk</env>
      <env name="PIO_VERSION_MAJOR">2</env>
      <env name="PIO_LIBDIR">/global/cfs/cdirs/ccsm1/parallelio/2.6.3/2023.2.0/8.1.28/lib</env>
      <env name="PIO_INCDIR">/global/cfs/cdirs/ccsm1/parallelio/2.6.3/2023.2.0/8.1.28/include</env>
      <env name="PIO_TYPENAME_VALID_VALUES">pnetcdf,netcdf,netcdf4p</env>
      <env name="ESMF_RUNTIME_PROFILE">ON</env>
      <env name="ESMF_RUNTIME_PROFILE_OUTPUT">SUMMARY</env>
    </environment_variables>
  </machine>
