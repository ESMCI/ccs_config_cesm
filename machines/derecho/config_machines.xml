<machine MACH="derecho">
    <DESC>NCAR AMD EPYC system</DESC>
    <OS>CNL</OS>
    <COMPILERS>intel,gnu,nvhpc,cray</COMPILERS>
    <MPILIBS>mpich,openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{CESMDATAROOT}/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CESMDATAROOT}/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>16</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MEM_PER_TASK>10</MEM_PER_TASK>
    <MAX_MEM_PER_NODE>235</MAX_MEM_PER_NODE>
    <MAX_GPUS_PER_NODE>4</MAX_GPUS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <MAX_CPUTASKS_PER_GPU_NODE>64</MAX_CPUTASKS_PER_GPU_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <TORCH_DIR>/glade/u/apps/cseg/derecho/conda/cesm_pyenv/1.0</TORCH_DIR>
    <mpirun mpilib="default">
      <executable>mpibind</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="buffer"> --line-buffer</arg>
        <arg name="separator"> -- </arg>
      </arguments>
   </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">$ENV{LMOD_ROOT}/lmod/init/perl</init_path>
      <init_path lang="python">$ENV{LMOD_ROOT}/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">$ENV{LMOD_ROOT}/lmod/init/sh</init_path>
      <init_path lang="csh">$ENV{LMOD_ROOT}/lmod/init/csh</init_path>
      <cmd_path lang="perl">$ENV{LMOD_ROOT}/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">$ENV{LMOD_ROOT}/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="load">cesmdev/1.0</command>
        <command name="load">ncarenv/25.10</command>
        <command name="purge"/>
        <command name="load">conda/latest</command>
        <command name="load">nco</command>
        <command name="load">craype</command>
        <command name="load">cmake</command>
      </modules>
      <modules compiler="intel">
        <command name="load">intel/2025.2.1</command>
        <command name="load">mkl</command>
<!--        <command name="load">kokkos/4.2.01</command> -->
      </modules>
      <modules compiler="cray">
        <command name="load">cce/19.0.0</command>
        <command name="load">cray-libsci/25.03.0</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gcc/14.3.0</command>
        <command name="load">cray-libsci/25.03.0</command>
      </modules>
      <modules compiler="nvhpc">
        <command name="load">nvhpc/25.9</command>
        <command name="load">cray-libsci/25.03.0</command>
      </modules>
      <modules>
        <command name="load">ncarcompilers/1.1.0</command>
      </modules>
<!--      <modules mpilib="openmpi" compiler="!gnu">
        <command name="load">openmpi/5.0.7</command>
      </modules> -->
      <modules mpilib="mpich">
        <command name="load">cray-mpich/8.1.32</command>
      </modules>
      <modules mpilib="mpich" compiler="nvhpc" gpu_type="!none">
        <command name="load">cuda/12.9</command>
      </modules>
      <modules mpilib="mpich" compiler="gnu" gpu_type="!none">
        <command name="load">cuda/12.9</command>
      </modules>

      <!-- NetCDF, mpi-serial if NOT full MPI and pnetcdf libraries -->

      <!-- libraries with mpi-serial MPI library -->
      <modules mpilib="mpi-serial">
        <command name="load">netcdf/4.9.3</command>
        <command name="load">mpi-serial/2.5.0</command>
        <command name="load">parallelio-serial/2.6.6</command>
      </modules>

      <!-- libraries with full MPI library -->
      <modules mpilib="!mpi-serial">
        <command name="load">netcdf-mpi/4.9.3</command>
        <command name="load">parallel-netcdf/1.14.1</command>
        <command name="load">parallelio/2.6.6</command>
      </modules>

      <!-- ESMF libraries -->

      <modules>
        <command name="load">esmf/8.9.0</command>
      </modules>
    </module_system>

    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="FI_CXI_RX_MATCH_MODE">hybrid</env>
      <!-- https://support.hpe.com/hpesc/public/docDisplay?docId=a00129146en_us&docLocale=en_US -->
      <env name="FI_MR_CACHE_MONITOR">memhooks</env>
    </environment_variables>
    <!-- derecho has both gpfs and lustre file systems so I think this setting may cause issues -->
    <!--    <environment_variables mpilib="mpich">
         <env name="MPICH_MPIIO_HINTS">*:romio_cb_read=enable:romio_cb_write=enable:striping_factor=2</env>
         </environment_variables>
      -->
    <environment_variables comp_interface="nuopc">
      <!-- required on all systems for timing file output -->
      <env name="ESMF_RUNTIME_PROFILE">ON</env>
      <env name="ESMF_RUNTIME_PROFILE_OUTPUT">SUMMARY</env>
    </environment_variables>
    <environment_variables gpu_type="!none">
      <env name="NCAR_LIBS_CUDA">-lcuda -lcudart</env>
    </environment_variables>
  </machine>
