  <machine MACH="greenplanet-sky24">
    <DESC>UCI Linux Cluster; 40 pes/node, batch system is slurm</DESC>
    <OS>LINUX</OS>
    <!-- PROXY: optional http proxy for access to the internet-->
    <!-- <PROXY> https://howto.get.out </PROXY> -->
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <!-- PROJECT: A project or account number used for batch jobs
         This value is used for directory names. If different from
         actual accounting project id, use CHARGE_ACCOUNT
         can be overridden in environment or $HOME/.cime/config -->
    <!-- <PROJECT>couldbethis</PROJECT> -->

    <!-- CHARGE_ACCOUNT: A project or account number used for batch jobs
         This is the actual project used for cost accounting set in
         the batch script (ex. #PBS -A charge_account). Will default
         to PROJECT if not set.
         can be overridden in environment or $HOME/.cime/config -->
    <!-- <CHARGE_ACCOUNT>couldbethis</CHARGE_ACCOUNT> -->
    <CIME_OUTPUT_ROOT>/DFS-L/SCRATCH/moore/$USER/cesm_runs</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/DFS-L/DATA/moore/cesm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/DFS-L/DATA/moore/cesm/inputdata</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/DFS-L/DATA/moore/cesm/baselines</BASELINE_ROOT>
    <!-- CCSM_CPRNC: location of the cprnc tool, compares model output in testing-->
    <CCSM_CPRNC>/DFS-L/DATA/moore/cesm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE>gmake</GMAKE>
    <GMAKE_J>16</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>mlevy@ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>40</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>40</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>

    <mpirun mpilib="default">
      <!-- name of the exectuable used to launch mpi jobs -->
      <executable>mpirun</executable>
      <arguments>
        <arg name="task_count">-np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="task_count">-np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/usr/share/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
      </modules>
      <modules compiler="intel">
        <command name="load">intel/2018.3</command>
        <command name="load">netcdf/4.7.0</command>
      </modules>
      <modules mpilib="openmpi">
        <command name="load">openmpi/3.1.6</command>
        <command name="load">pnetcdf/1.10.0</command>
      </modules>
    </module_system>
<!-- environment variables, a blank entry will unset a variable -->
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="MPI_TYPE_DEPTH">16</env>
    </environment_variables>
    <!-- resource settings as defined in https://docs.python.org/2/library/resource.html -->
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

